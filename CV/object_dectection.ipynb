{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### face mask detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting and preparing the data\n",
    "\n",
    "Since we are building an object detector from scratch, we cannot use a pre-built model or transfer learning neither. This means that we need to train everything from scratch, starting from the model weights random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\a27_YEARS_OLD\\\\deep_learning\\\\CV'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"D:/a27_YEARS_OLD/deep_learning/CV/labeled-mask-dataset-yolo-darknet\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:/a27_YEARS_OLD/deep_learning/CV/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labeled-mask-dataset-yolo-darknet/versions/1/obj/0.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded = 0\n",
    "masked_instance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = 'labeled-mask-dataset-yolo-darknet/versions/1/' # obj/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, d, f in os.walk(full_data_path): # walk through data path to find content\n",
    "    # print(r)\n",
    "    # print(d)\n",
    "    # print(f)\n",
    "    for file in f:\n",
    "        # print(file)\n",
    "        if file.endswith(\".txt\"):\n",
    "            # first, let's check if there is only one object\n",
    "            with open( full_data_path + 'obj/' + file, 'r') as fp: \n",
    "                lines = fp.readlines()\n",
    "                if len(lines) > 1:\n",
    "                    discarded += 1\n",
    "                    continue\n",
    "            # print(file)\n",
    "\n",
    "            strip = file[0:len(file) - len(\".txt\")]  \n",
    "            # print(strip)\n",
    "            # secondly, check if the paired image actually exist\n",
    "            image_path = full_data_path + \"obj/\" + strip + '.jpg'\n",
    "            text_path = full_data_path + \"obj/\" + strip + '.txt'\n",
    "            # print(image_path)\n",
    "\n",
    "            if os.path.isfile(text_path):\n",
    "                # checking the class. '0' means masked, '1' for unmasked\n",
    "                # print('x')  \n",
    "                if lines[0][0] == '0': # 0 0.5752380952380952 0.4142857142857143 0.21714285714285714 0.3485714285714286\n",
    "                    masked_instance += 1\n",
    "                files.append(strip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = full_data_path + \"obj/\" + strip + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labeled-mask-dataset-yolo-darknet/versions/1/obj/with_maskb (9).txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 file(s) discarded\n",
      "1292 valid case(s)\n",
      "832 are masked cases\n"
     ]
    }
   ],
   "source": [
    "\n",
    "size = len(files)   \n",
    "print(str(discarded) + \" file(s) discarded\")\n",
    "print(str(size) + \" valid case(s)\")\n",
    "print(str(masked_instance) + \" are masked cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-with-mask',\n",
       " '0',\n",
       " '1-with-mask',\n",
       " '10-with-mask',\n",
       " '100-with-mask',\n",
       " '101-with-mask',\n",
       " '103-with-mask',\n",
       " '104-with-mask',\n",
       " '105-with-mask',\n",
       " '106-with-mask',\n",
       " '107-with-mask',\n",
       " '108-with-mask',\n",
       " '109-with-mask',\n",
       " '11-with-mask',\n",
       " '110-with-mask',\n",
       " '111-with-mask',\n",
       " '112-with-mask',\n",
       " '113-with-mask',\n",
       " '114-with-mask',\n",
       " '115-with-mask',\n",
       " '116-with-mask',\n",
       " '117-with-mask',\n",
       " '118-with-mask',\n",
       " '119-with-mask',\n",
       " '12-with-mask',\n",
       " '120-with-mask',\n",
       " '121-with-mask',\n",
       " '122-with-mask',\n",
       " '123-with-mask',\n",
       " '124-with-mask',\n",
       " '125-with-mask',\n",
       " '126-with-mask',\n",
       " '127-with-mask',\n",
       " '128-with-mask',\n",
       " '129-with-mask',\n",
       " '13-with-mask',\n",
       " '131-with-mask',\n",
       " '132-with-mask',\n",
       " '132',\n",
       " '133-with-mask',\n",
       " '134-with-mask',\n",
       " '134',\n",
       " '135-with-mask',\n",
       " '136-with-mask',\n",
       " '137-with-mask',\n",
       " '137',\n",
       " '138-with-mask',\n",
       " '138',\n",
       " '139-with-mask',\n",
       " '14-with-mask',\n",
       " '140-with-mask',\n",
       " '141-with-mask',\n",
       " '144-with-mask',\n",
       " '146-with-mask',\n",
       " '147-with-mask',\n",
       " '148-with-mask',\n",
       " '148',\n",
       " '149',\n",
       " '150-with-mask',\n",
       " '151-with-mask',\n",
       " '151',\n",
       " '152-with-mask',\n",
       " '153-with-mask',\n",
       " '154-with-mask',\n",
       " '155-with-mask',\n",
       " '156-with-mask',\n",
       " '157-with-mask',\n",
       " '158-with-mask',\n",
       " '159-with-mask',\n",
       " '159',\n",
       " '160-with-mask',\n",
       " '161-with-mask',\n",
       " '162-with-mask',\n",
       " '163-with-mask',\n",
       " '165-with-mask',\n",
       " '167-with-mask',\n",
       " '168-with-mask',\n",
       " '169-with-mask',\n",
       " '169',\n",
       " '170-with-mask',\n",
       " '171-with-mask',\n",
       " '172-with-mask',\n",
       " '173-with-mask',\n",
       " '174-with-mask',\n",
       " '175-with-mask',\n",
       " '176-with-mask',\n",
       " '177-with-mask',\n",
       " '178',\n",
       " '179-with-mask',\n",
       " '180-with-mask',\n",
       " '181-with-mask',\n",
       " '182-with-mask',\n",
       " '183-with-mask',\n",
       " '184-with-mask',\n",
       " '185-with-mask',\n",
       " '186-with-mask',\n",
       " '187-with-mask',\n",
       " '189-with-mask',\n",
       " '190-with-mask',\n",
       " '192-with-mask',\n",
       " '193-with-mask',\n",
       " '194-with-mask',\n",
       " '195-with-mask',\n",
       " '195',\n",
       " '196-with-mask',\n",
       " '197-with-mask',\n",
       " '198-with-mask',\n",
       " '2-with-mask',\n",
       " '200-with-mask',\n",
       " '201-with-mask',\n",
       " '201',\n",
       " '202-with-mask',\n",
       " '203-with-mask',\n",
       " '204-with-mask',\n",
       " '205-with-mask',\n",
       " '206-with-mask',\n",
       " '207-with-mask',\n",
       " '209-with-mask',\n",
       " '211-with-mask',\n",
       " '212-with-mask',\n",
       " '214-with-mask',\n",
       " '215-with-mask',\n",
       " '217-with-mask',\n",
       " '218-with-mask',\n",
       " '219-with-mask',\n",
       " '221-with-mask',\n",
       " '222-with-mask',\n",
       " '223-with-mask',\n",
       " '224-with-mask',\n",
       " '225-with-mask',\n",
       " '227-with-mask',\n",
       " '228-with-mask',\n",
       " '229-with-mask',\n",
       " '231-with-mask',\n",
       " '232-with-mask',\n",
       " '233-with-mask',\n",
       " '234-with-mask',\n",
       " '235-with-mask',\n",
       " '236-with-mask',\n",
       " '237-with-mask',\n",
       " '238-with-mask',\n",
       " '239-with-mask',\n",
       " '240-with-mask',\n",
       " '241-with-mask',\n",
       " '242-with-mask',\n",
       " '243-with-mask',\n",
       " '244-with-mask',\n",
       " '245-with-mask',\n",
       " '246-with-mask',\n",
       " '251-with-mask',\n",
       " '252-with-mask',\n",
       " '253-with-mask',\n",
       " '254-with-mask',\n",
       " '256-with-mask',\n",
       " '257-with-mask',\n",
       " '259-with-mask',\n",
       " '26-with-mask',\n",
       " '260-with-mask',\n",
       " '261-with-mask',\n",
       " '262-with-mask',\n",
       " '263-with-mask',\n",
       " '267-with-mask',\n",
       " '268-with-mask',\n",
       " '269-with-mask',\n",
       " '27-with-mask',\n",
       " '270-with-mask',\n",
       " '272-with-mask',\n",
       " '273-with-mask',\n",
       " '274-with-mask',\n",
       " '275-with-mask',\n",
       " '276-with-mask',\n",
       " '277-with-mask',\n",
       " '278-with-mask',\n",
       " '279-with-mask',\n",
       " '281-with-mask',\n",
       " '282-with-mask',\n",
       " '283-with-mask',\n",
       " '284-with-mask',\n",
       " '285-with-mask',\n",
       " '286-with-mask',\n",
       " '287-with-mask',\n",
       " '288-with-mask',\n",
       " '289-with-mask',\n",
       " '29-with-mask',\n",
       " '290-with-mask',\n",
       " '291-with-mask',\n",
       " '293-with-mask',\n",
       " '294-with-mask',\n",
       " '295-with-mask',\n",
       " '296-with-mask',\n",
       " '298-with-mask',\n",
       " '299-with-mask',\n",
       " '3-with-mask',\n",
       " '30-with-mask',\n",
       " '301-with-mask',\n",
       " '302-with-mask',\n",
       " '303-with-mask',\n",
       " '304-with-mask',\n",
       " '305-with-mask',\n",
       " '306-with-mask',\n",
       " '307-with-mask',\n",
       " '308-with-mask',\n",
       " '309-with-mask',\n",
       " '31-with-mask',\n",
       " '311-with-mask',\n",
       " '313-with-mask',\n",
       " '314-with-mask',\n",
       " '315-with-mask',\n",
       " '316-with-mask',\n",
       " '317-with-mask',\n",
       " '318-with-mask',\n",
       " '319-with-mask',\n",
       " '32-with-mask',\n",
       " '322-with-mask',\n",
       " '323-with-mask',\n",
       " '324-with-mask',\n",
       " '325-with-mask',\n",
       " '326-with-mask',\n",
       " '327-with-mask',\n",
       " '33-with-mask',\n",
       " '330-with-mask',\n",
       " '331-with-mask',\n",
       " '332-with-mask',\n",
       " '332',\n",
       " '333-with-mask',\n",
       " '334-with-mask',\n",
       " '335-with-mask',\n",
       " '336-with-mask',\n",
       " '338-with-mask',\n",
       " '339-with-mask',\n",
       " '34-with-mask',\n",
       " '340-with-mask',\n",
       " '342-with-mask',\n",
       " '343-with-mask',\n",
       " '344-with-mask',\n",
       " '345-with-mask',\n",
       " '346-with-mask',\n",
       " '347-with-mask',\n",
       " '348-with-mask',\n",
       " '349-with-mask',\n",
       " '35-with-mask',\n",
       " '351-with-mask',\n",
       " '353-with-mask',\n",
       " '355-with-mask',\n",
       " '356-with-mask',\n",
       " '357-with-mask',\n",
       " '358-with-mask',\n",
       " '359-with-mask',\n",
       " '36-with-mask',\n",
       " '362-with-mask',\n",
       " '364-with-mask',\n",
       " '365-with-mask',\n",
       " '366-with-mask',\n",
       " '368-with-mask',\n",
       " '369-with-mask',\n",
       " '37-with-mask',\n",
       " '374-with-mask',\n",
       " '375-with-mask',\n",
       " '376-with-mask',\n",
       " '377-with-mask',\n",
       " '378-with-mask',\n",
       " '379-with-mask',\n",
       " '38-with-mask',\n",
       " '381-with-mask',\n",
       " '382-with-mask',\n",
       " '383-with-mask',\n",
       " '384-with-mask',\n",
       " '385-with-mask',\n",
       " '386-with-mask',\n",
       " '387-with-mask',\n",
       " '39-with-mask',\n",
       " '390-with-mask',\n",
       " '391-with-mask',\n",
       " '392-with-mask',\n",
       " '393-with-mask',\n",
       " '394-with-mask',\n",
       " '395-with-mask',\n",
       " '396-with-mask',\n",
       " '397-with-mask',\n",
       " '398-with-mask',\n",
       " '399-with-mask',\n",
       " '4-with-mask',\n",
       " '40-with-mask',\n",
       " '401-with-mask',\n",
       " '402-with-mask',\n",
       " '404-with-mask',\n",
       " '405-with-mask',\n",
       " '406-with-mask',\n",
       " '407-with-mask',\n",
       " '408-with-mask',\n",
       " '409-with-mask',\n",
       " '41-with-mask',\n",
       " '411-with-mask',\n",
       " '413-with-mask',\n",
       " '414-with-mask',\n",
       " '415-with-mask',\n",
       " '416-with-mask',\n",
       " '417-with-mask',\n",
       " '418-with-mask',\n",
       " '419-with-mask',\n",
       " '42-with-mask',\n",
       " '420-with-mask',\n",
       " '421-with-mask',\n",
       " '422-with-mask',\n",
       " '425-with-mask',\n",
       " '427-with-mask',\n",
       " '429-with-mask',\n",
       " '43-with-mask',\n",
       " '430-with-mask',\n",
       " '431-with-mask',\n",
       " '431',\n",
       " '432-with-mask',\n",
       " '433-with-mask',\n",
       " '434-with-mask',\n",
       " '435-with-mask',\n",
       " '436-with-mask',\n",
       " '438-with-mask',\n",
       " '439-with-mask',\n",
       " '440-with-mask',\n",
       " '442-with-mask',\n",
       " '443-with-mask',\n",
       " '444-with-mask',\n",
       " '445-with-mask',\n",
       " '446-with-mask',\n",
       " '447-with-mask',\n",
       " '448-with-mask',\n",
       " '449-with-mask',\n",
       " '45-with-mask',\n",
       " '450-with-mask',\n",
       " '451-with-mask',\n",
       " '452-with-mask',\n",
       " '453-with-mask',\n",
       " '454-with-mask',\n",
       " '455-with-mask',\n",
       " '456-with-mask',\n",
       " '457-with-mask',\n",
       " '458-with-mask',\n",
       " '459-with-mask',\n",
       " '46-with-mask',\n",
       " '460-with-mask',\n",
       " '461-with-mask',\n",
       " '462-with-mask',\n",
       " '463-with-mask',\n",
       " '464-with-mask',\n",
       " '466-with-mask',\n",
       " '467-with-mask',\n",
       " '47-with-mask',\n",
       " '471-with-mask',\n",
       " '472-with-mask',\n",
       " '473-with-mask',\n",
       " '474-with-mask',\n",
       " '475-with-mask',\n",
       " '476-with-mask',\n",
       " '477-with-mask',\n",
       " '478-with-mask',\n",
       " '479-with-mask',\n",
       " '48-with-mask',\n",
       " '480-with-mask',\n",
       " '481-with-mask',\n",
       " '49-with-mask',\n",
       " '49',\n",
       " '5-with-mask',\n",
       " '50-with-mask',\n",
       " '51-with-mask',\n",
       " '52-with-mask',\n",
       " '53-with-mask',\n",
       " '56-with-mask',\n",
       " '57-with-mask',\n",
       " '59-with-mask',\n",
       " '6-with-mask',\n",
       " '61-with-mask',\n",
       " '62-with-mask',\n",
       " '63',\n",
       " '64-with-mask',\n",
       " '65-with-mask',\n",
       " '67-with-mask',\n",
       " '68-with-mask',\n",
       " '7-with-mask',\n",
       " '70-with-mask',\n",
       " '71-with-mask',\n",
       " '72-with-mask',\n",
       " '73-with-mask',\n",
       " '74-with-mask',\n",
       " '75-with-mask',\n",
       " '76-with-mask',\n",
       " '77-with-mask',\n",
       " '78-with-mask',\n",
       " '79-with-mask',\n",
       " '8-with-mask',\n",
       " '80-with-mask',\n",
       " '81-with-mask',\n",
       " '82-with-mask',\n",
       " '83-with-mask',\n",
       " '84-with-mask',\n",
       " '85-with-mask',\n",
       " '86-with-mask',\n",
       " '88-with-mask',\n",
       " '89-with-mask',\n",
       " '9-with-mask',\n",
       " '91-with-mask',\n",
       " '92-with-mask',\n",
       " '93-with-mask',\n",
       " '94-with-mask',\n",
       " '96-with-mask',\n",
       " '97-with-mask',\n",
       " '98-with-mask',\n",
       " '99-with-mask',\n",
       " 'masked (10)',\n",
       " 'masked (11)',\n",
       " 'masked (1257)',\n",
       " 'masked (1258)',\n",
       " 'masked (1259)',\n",
       " 'masked (1260)',\n",
       " 'masked (1261)',\n",
       " 'masked (1263)',\n",
       " 'masked (1265)',\n",
       " 'masked (1266)',\n",
       " 'masked (1267)',\n",
       " 'masked (1268)',\n",
       " 'masked (1269)',\n",
       " 'masked (1270)',\n",
       " 'masked (1271)',\n",
       " 'masked (1272)',\n",
       " 'masked (1273)',\n",
       " 'masked (1274)',\n",
       " 'masked (1275)',\n",
       " 'masked (1276)',\n",
       " 'masked (1277)',\n",
       " 'masked (1278)',\n",
       " 'masked (1279)',\n",
       " 'masked (1280)',\n",
       " 'masked (1281)',\n",
       " 'masked (1282)',\n",
       " 'masked (1283)',\n",
       " 'masked (1284)',\n",
       " 'masked (1285)',\n",
       " 'masked (1286)',\n",
       " 'masked (1287)',\n",
       " 'masked (1288)',\n",
       " 'masked (1289)',\n",
       " 'masked (1290)',\n",
       " 'masked (1291)',\n",
       " 'masked (1292)',\n",
       " 'masked (1293)',\n",
       " 'masked (1296)',\n",
       " 'masked (1297)',\n",
       " 'masked (1298)',\n",
       " 'masked (1299)',\n",
       " 'masked (1300)',\n",
       " 'masked (1301)',\n",
       " 'masked (1302)',\n",
       " 'masked (1303)',\n",
       " 'masked (1304)',\n",
       " 'masked (1305)',\n",
       " 'masked (1306)',\n",
       " 'masked (1307)',\n",
       " 'masked (1308)',\n",
       " 'masked (1309)',\n",
       " 'masked (1310)',\n",
       " 'masked (1311)',\n",
       " 'masked (1312)',\n",
       " 'masked (1313)',\n",
       " 'masked (1314)',\n",
       " 'masked (1315)',\n",
       " 'masked (1316)',\n",
       " 'masked (1317)',\n",
       " 'masked (1318)',\n",
       " 'masked (1319)',\n",
       " 'masked (1320)',\n",
       " 'masked (1321)',\n",
       " 'masked (1322)',\n",
       " 'masked (1323)',\n",
       " 'masked (1324)',\n",
       " 'masked (1325)',\n",
       " 'masked (1326)',\n",
       " 'masked (1327)',\n",
       " 'masked (1328)',\n",
       " 'masked (1329)',\n",
       " 'masked (1330)',\n",
       " 'masked (1331)',\n",
       " 'masked (1332)',\n",
       " 'masked (1333)',\n",
       " 'masked (1334)',\n",
       " 'masked (1335)',\n",
       " 'masked (1336)',\n",
       " 'masked (1337)',\n",
       " 'masked (1338)',\n",
       " 'masked (1339)',\n",
       " 'masked (1340)',\n",
       " 'masked (1341)',\n",
       " 'masked (1342)',\n",
       " 'masked (1343)',\n",
       " 'masked (1344)',\n",
       " 'masked (1345)',\n",
       " 'masked (1346)',\n",
       " 'masked (1347)',\n",
       " 'masked (1348)',\n",
       " 'masked (1349)',\n",
       " 'masked (1350)',\n",
       " 'masked (1351)',\n",
       " 'masked (1352)',\n",
       " 'masked (1353)',\n",
       " 'masked (1354)',\n",
       " 'masked (1355)',\n",
       " 'masked (1356)',\n",
       " 'masked (1357)',\n",
       " 'masked (1358)',\n",
       " 'masked (1360)',\n",
       " 'masked (1361)',\n",
       " 'masked (1362)',\n",
       " 'masked (1363)',\n",
       " 'masked (1364)',\n",
       " 'masked (1365)',\n",
       " 'masked (1366)',\n",
       " 'masked (1367)',\n",
       " 'masked (1369)',\n",
       " 'masked (1370)',\n",
       " 'masked (1371)',\n",
       " 'masked (1373)',\n",
       " 'masked (1375)',\n",
       " 'masked (1376)',\n",
       " 'masked (1377)',\n",
       " 'masked (1378)',\n",
       " 'masked (1380)',\n",
       " 'masked (1381)',\n",
       " 'masked (1382)',\n",
       " 'masked (1383)',\n",
       " 'masked (1384)',\n",
       " 'masked (1385)',\n",
       " 'masked (1386)',\n",
       " 'masked (1387)',\n",
       " 'masked (1388)',\n",
       " 'masked (1389)',\n",
       " 'masked (1390)',\n",
       " 'masked (1391)',\n",
       " 'masked (1392)',\n",
       " 'masked (1393)',\n",
       " 'masked (1394)',\n",
       " 'masked (1395)',\n",
       " 'masked (1396)',\n",
       " 'masked (1397)',\n",
       " 'masked (1399)',\n",
       " 'masked (1400)',\n",
       " 'masked (1401)',\n",
       " 'masked (1402)',\n",
       " 'masked (1403)',\n",
       " 'masked (1404)',\n",
       " 'masked (1405)',\n",
       " 'masked (1406)',\n",
       " 'masked (1407)',\n",
       " 'masked (1408)',\n",
       " 'masked (1409)',\n",
       " 'masked (1410)',\n",
       " 'masked (1411)',\n",
       " 'masked (1412)',\n",
       " 'masked (1413)',\n",
       " 'masked (1414)',\n",
       " 'masked (1415)',\n",
       " 'masked (1416)',\n",
       " 'masked (1417)',\n",
       " 'masked (1418)',\n",
       " 'masked (1419)',\n",
       " 'masked (1420)',\n",
       " 'masked (1421)',\n",
       " 'masked (1422)',\n",
       " 'masked (1423)',\n",
       " 'masked (1424)',\n",
       " 'masked (1426)',\n",
       " 'masked (1427)',\n",
       " 'masked (1429)',\n",
       " 'masked (1430)',\n",
       " 'masked (1431)',\n",
       " 'masked (1432)',\n",
       " 'masked (1433)',\n",
       " 'masked (1434)',\n",
       " 'masked (1435)',\n",
       " 'masked (1436)',\n",
       " 'masked (1437)',\n",
       " 'masked (1438)',\n",
       " 'masked (1439)',\n",
       " 'masked (1440)',\n",
       " 'masked (1441)',\n",
       " 'masked (1442)',\n",
       " 'masked (1443)',\n",
       " 'masked (1444)',\n",
       " 'masked (1445)',\n",
       " 'masked (1446)',\n",
       " 'masked (1447)',\n",
       " 'masked (1448)',\n",
       " 'masked (1449)',\n",
       " 'masked (1450)',\n",
       " 'masked (1451)',\n",
       " 'masked (1456)',\n",
       " 'masked (1457)',\n",
       " 'masked (1458)',\n",
       " 'masked (1459)',\n",
       " 'masked (1460)',\n",
       " 'masked (1461)',\n",
       " 'masked (1462)',\n",
       " 'masked (1463)',\n",
       " 'masked (1464)',\n",
       " 'masked (1465)',\n",
       " 'masked (1467)',\n",
       " 'masked (1468)',\n",
       " 'masked (1469)',\n",
       " 'masked (1470)',\n",
       " 'masked (1471)',\n",
       " 'masked (1472)',\n",
       " 'masked (1473)',\n",
       " 'masked (1474)',\n",
       " 'masked (1475)',\n",
       " 'masked (1476)',\n",
       " 'masked (1477)',\n",
       " 'masked (1478)',\n",
       " 'masked (1479)',\n",
       " 'masked (1481)',\n",
       " 'masked (1482)',\n",
       " 'masked (1483)',\n",
       " 'masked (1485)',\n",
       " 'masked (1486)',\n",
       " 'masked (1487)',\n",
       " 'masked (1488)',\n",
       " 'masked (1489)',\n",
       " 'masked (1490)',\n",
       " 'masked (1491)',\n",
       " 'masked (1492)',\n",
       " 'masked (1493)',\n",
       " 'masked (1494)',\n",
       " 'masked (1496)',\n",
       " 'masked (1497)',\n",
       " 'masked (1498)',\n",
       " 'masked (1499)',\n",
       " 'masked (1500)',\n",
       " 'masked (1501)',\n",
       " 'masked (1502)',\n",
       " 'masked (1503)',\n",
       " 'masked (1504)',\n",
       " 'masked (1506)',\n",
       " 'masked (1507)',\n",
       " 'masked (1508)',\n",
       " 'masked (1509)',\n",
       " 'masked (1511)',\n",
       " 'masked (1512)',\n",
       " 'masked (1513)',\n",
       " 'masked (1514)',\n",
       " 'masked (1515)',\n",
       " 'masked (1516)',\n",
       " 'masked (1517)',\n",
       " 'masked (1518)',\n",
       " 'masked (1519)',\n",
       " 'masked (1520)',\n",
       " 'masked (1521)',\n",
       " 'masked (1522)',\n",
       " 'masked (1524)',\n",
       " 'masked (1525)',\n",
       " 'masked (1526)',\n",
       " 'masked (1527)',\n",
       " 'masked (1528)',\n",
       " 'masked (1530)',\n",
       " 'masked (1531)',\n",
       " 'masked (1532)',\n",
       " 'masked (1534)',\n",
       " 'masked (1538)',\n",
       " 'masked (1539)',\n",
       " 'masked (1542)',\n",
       " 'masked (1544)',\n",
       " 'masked (1546)',\n",
       " 'masked (1547)',\n",
       " 'masked (1548)',\n",
       " 'masked (1550)',\n",
       " 'masked (1551)',\n",
       " 'masked (1552)',\n",
       " 'masked (1557)',\n",
       " 'masked (1558)',\n",
       " 'masked (1559)',\n",
       " 'masked (1560)',\n",
       " 'masked (1562)',\n",
       " 'masked (1563)',\n",
       " 'masked (1564)',\n",
       " 'masked (1565)',\n",
       " 'masked (1566)',\n",
       " 'masked (1567)',\n",
       " 'masked (1568)',\n",
       " 'masked (1569)',\n",
       " 'masked (1572)',\n",
       " 'masked (1573)',\n",
       " 'masked (1575)',\n",
       " 'masked (1576)',\n",
       " 'masked (1577)',\n",
       " 'masked (1578)',\n",
       " 'masked (1579)',\n",
       " 'masked (1580)',\n",
       " 'masked (1581)',\n",
       " 'masked (1583)',\n",
       " 'masked (1584)',\n",
       " 'masked (1585)',\n",
       " 'masked (1586)',\n",
       " 'masked (1588)',\n",
       " 'masked (1589)',\n",
       " 'masked (1590)',\n",
       " 'masked (1591)',\n",
       " 'masked (1592)',\n",
       " 'masked (1593)',\n",
       " 'masked (1595)',\n",
       " 'masked (1596)',\n",
       " 'masked (1598)',\n",
       " 'masked (1600)',\n",
       " 'masked (1601)',\n",
       " 'masked (1602)',\n",
       " 'masked (1603)',\n",
       " 'masked (1604)',\n",
       " 'masked (1606)',\n",
       " 'masked (1607)',\n",
       " 'masked (1608)',\n",
       " 'masked (1611)',\n",
       " 'masked (1612)',\n",
       " 'masked (1613)',\n",
       " 'masked (1615)',\n",
       " 'masked (1616)',\n",
       " 'masked (1617)',\n",
       " 'masked (1619)',\n",
       " 'masked (1620)',\n",
       " 'masked (1621)',\n",
       " 'masked (1622)',\n",
       " 'masked (1623)',\n",
       " 'masked (1625)',\n",
       " 'masked (1626)',\n",
       " 'masked (1628)',\n",
       " 'masked (1633)',\n",
       " 'masked (1634)',\n",
       " 'masked (1635)',\n",
       " 'masked (1636)',\n",
       " 'masked (1637)',\n",
       " 'masked (1638)',\n",
       " 'masked (1639)',\n",
       " 'masked (1641)',\n",
       " 'masked (1644)',\n",
       " 'masked (1645)',\n",
       " 'masked (1647)',\n",
       " 'masked (1648)',\n",
       " 'masked (1649)',\n",
       " 'masked (1650)',\n",
       " 'masked (1651)',\n",
       " 'masked (1652)',\n",
       " 'masked (1653)',\n",
       " 'masked (1655)',\n",
       " 'masked (1656)',\n",
       " 'masked (1658)',\n",
       " 'masked (1660)',\n",
       " 'masked (1661)',\n",
       " 'masked (1663)',\n",
       " 'masked (1664)',\n",
       " 'masked (1665)',\n",
       " 'masked (1666)',\n",
       " 'masked (1668)',\n",
       " 'masked (1669)',\n",
       " 'masked (1670)',\n",
       " 'masked (1671)',\n",
       " 'masked (1672)',\n",
       " 'masked (1673)',\n",
       " 'masked (1675)',\n",
       " 'masked (1677)',\n",
       " 'masked (1678)',\n",
       " 'masked (1680)',\n",
       " 'masked (1681)',\n",
       " 'masked (1683)',\n",
       " 'masked (1684)',\n",
       " 'masked (1686)',\n",
       " 'masked (1688)',\n",
       " 'masked (1691)',\n",
       " 'masked (1692)',\n",
       " 'masked (1693)',\n",
       " 'masked (1694)',\n",
       " 'masked (1695)',\n",
       " 'masked (1696)',\n",
       " 'masked (1697)',\n",
       " 'masked (1700)',\n",
       " 'masked (1701)',\n",
       " 'masked (1862)',\n",
       " 'masked (1867)',\n",
       " 'masked (1868)',\n",
       " 'masked (1869)',\n",
       " 'masked (1871)',\n",
       " 'masked (1872)',\n",
       " 'masked (1873)',\n",
       " 'masked (1874)',\n",
       " 'masked (1875)',\n",
       " 'masked (1876)',\n",
       " 'masked (1877)',\n",
       " 'masked (1878)',\n",
       " 'masked (1879)',\n",
       " 'masked (1880)',\n",
       " 'masked (1881)',\n",
       " 'masked (1883)',\n",
       " 'masked (1884)',\n",
       " 'masked (1887)',\n",
       " 'masked (1889)',\n",
       " 'masked (1890)',\n",
       " 'masked (1892)',\n",
       " 'masked (1895)',\n",
       " 'masked (1897)',\n",
       " 'masked (1898)',\n",
       " 'masked (1900)',\n",
       " 'masked (1901)',\n",
       " 'masked (1902)',\n",
       " 'masked (1903)',\n",
       " 'masked (1905)',\n",
       " 'masked (1907)',\n",
       " 'masked (1908)',\n",
       " 'masked (1909)',\n",
       " 'masked (1910)',\n",
       " 'masked (1912)',\n",
       " 'masked (1913)',\n",
       " 'masked (3)',\n",
       " 'masked (4)',\n",
       " 'masked (5)',\n",
       " 'masked (6)',\n",
       " 'masked (7)',\n",
       " 'masked (8)',\n",
       " 'masked (9)',\n",
       " 'pra1',\n",
       " 'pra2',\n",
       " 'pra3',\n",
       " 'prajna',\n",
       " 'unmasked (1250)',\n",
       " 'unmasked (1251)',\n",
       " 'unmasked (1252)',\n",
       " 'unmasked (1253)',\n",
       " 'unmasked (1255)',\n",
       " 'unmasked (1256)',\n",
       " 'unmasked (1257)',\n",
       " 'unmasked (1260)',\n",
       " 'unmasked (1261)',\n",
       " 'unmasked (1262)',\n",
       " 'unmasked (1263)',\n",
       " 'unmasked (1264)',\n",
       " 'unmasked (1265)',\n",
       " 'unmasked (1266)',\n",
       " 'unmasked (1267)',\n",
       " 'unmasked (1268)',\n",
       " 'unmasked (1269)',\n",
       " 'unmasked (1270)',\n",
       " 'unmasked (1271)',\n",
       " 'unmasked (1272)',\n",
       " 'unmasked (1274)',\n",
       " 'unmasked (1275)',\n",
       " 'unmasked (1276)',\n",
       " 'unmasked (1277)',\n",
       " 'unmasked (1278)',\n",
       " 'unmasked (1279)',\n",
       " 'unmasked (1280)',\n",
       " 'unmasked (1282)',\n",
       " 'unmasked (1285)',\n",
       " 'unmasked (1286)',\n",
       " 'unmasked (1287)',\n",
       " 'unmasked (1288)',\n",
       " 'unmasked (1289)',\n",
       " 'unmasked (1290)',\n",
       " 'unmasked (1291)',\n",
       " 'unmasked (1292)',\n",
       " 'unmasked (1293)',\n",
       " 'unmasked (1295)',\n",
       " 'unmasked (1297)',\n",
       " 'unmasked (1298)',\n",
       " 'unmasked (1299)',\n",
       " 'unmasked (1301)',\n",
       " 'unmasked (1302)',\n",
       " 'unmasked (1303)',\n",
       " 'unmasked (1304)',\n",
       " 'unmasked (1306)',\n",
       " 'unmasked (1307)',\n",
       " 'unmasked (1308)',\n",
       " 'unmasked (1309)',\n",
       " 'unmasked (1310)',\n",
       " 'unmasked (1311)',\n",
       " 'unmasked (1312)',\n",
       " 'unmasked (1313)',\n",
       " 'unmasked (1314)',\n",
       " 'unmasked (1315)',\n",
       " 'unmasked (1316)',\n",
       " 'unmasked (1317)',\n",
       " 'unmasked (1318)',\n",
       " 'unmasked (1319)',\n",
       " 'unmasked (1321)',\n",
       " 'unmasked (1322)',\n",
       " 'unmasked (1323)',\n",
       " 'unmasked (1324)',\n",
       " 'unmasked (1325)',\n",
       " 'unmasked (1326)',\n",
       " 'unmasked (1327)',\n",
       " 'unmasked (1328)',\n",
       " 'unmasked (1329)',\n",
       " 'unmasked (1330)',\n",
       " 'unmasked (1331)',\n",
       " 'unmasked (1332)',\n",
       " 'unmasked (1333)',\n",
       " 'unmasked (1334)',\n",
       " 'unmasked (1335)',\n",
       " 'unmasked (1336)',\n",
       " 'unmasked (1337)',\n",
       " 'unmasked (1338)',\n",
       " 'unmasked (1339)',\n",
       " 'unmasked (1340)',\n",
       " 'unmasked (1341)',\n",
       " 'unmasked (1342)',\n",
       " 'unmasked (1343)',\n",
       " 'unmasked (1344)',\n",
       " 'unmasked (1372)',\n",
       " 'unmasked (1374)',\n",
       " 'unmasked (1375)',\n",
       " 'unmasked (1376)',\n",
       " 'unmasked (1377)',\n",
       " 'unmasked (1378)',\n",
       " 'unmasked (1380)',\n",
       " 'unmasked (1381)',\n",
       " 'unmasked (1382)',\n",
       " 'unmasked (1383)',\n",
       " 'unmasked (1384)',\n",
       " 'unmasked (1385)',\n",
       " 'unmasked (1386)',\n",
       " 'unmasked (1387)',\n",
       " 'unmasked (1388)',\n",
       " 'unmasked (1389)',\n",
       " 'unmasked (1390)',\n",
       " 'unmasked (1391)',\n",
       " 'unmasked (1392)',\n",
       " 'unmasked (1393)',\n",
       " 'unmasked (1394)',\n",
       " 'unmasked (1395)',\n",
       " 'unmasked (1396)',\n",
       " 'unmasked (1397)',\n",
       " 'unmasked (1398)',\n",
       " 'unmasked (1399)',\n",
       " 'unmasked (1400)',\n",
       " 'unmasked (1401)',\n",
       " 'unmasked (1402)',\n",
       " 'unmasked (1403)',\n",
       " 'unmasked (1404)',\n",
       " 'unmasked (1405)',\n",
       " 'unmasked (1406)',\n",
       " 'unmasked (1407)',\n",
       " 'unmasked (1408)',\n",
       " 'unmasked (1409)',\n",
       " 'unmasked (1410)',\n",
       " 'unmasked (1411)',\n",
       " 'unmasked (1412)',\n",
       " 'unmasked (1413)',\n",
       " 'unmasked (1414)',\n",
       " 'unmasked (1415)',\n",
       " 'unmasked (1416)',\n",
       " 'unmasked (1417)',\n",
       " 'unmasked (1418)',\n",
       " 'unmasked (1419)',\n",
       " 'unmasked (1420)',\n",
       " 'unmasked (1421)',\n",
       " 'unmasked (1422)',\n",
       " 'unmasked (1424)',\n",
       " 'unmasked (1425)',\n",
       " 'unmasked (1426)',\n",
       " 'unmasked (1427)',\n",
       " 'unmasked (1428)',\n",
       " 'unmasked (1429)',\n",
       " 'unmasked (1430)',\n",
       " 'unmasked (1432)',\n",
       " 'unmasked (1433)',\n",
       " 'unmasked (1434)',\n",
       " 'unmasked (1435)',\n",
       " 'unmasked (1436)',\n",
       " 'unmasked (1437)',\n",
       " 'unmasked (1438)',\n",
       " 'unmasked (1439)',\n",
       " 'unmasked (1440)',\n",
       " 'unmasked (1441)',\n",
       " 'unmasked (1442)',\n",
       " 'unmasked (1443)',\n",
       " 'unmasked (1444)',\n",
       " 'unmasked (1445)',\n",
       " 'unmasked (1446)',\n",
       " 'unmasked (1447)',\n",
       " 'unmasked (1448)',\n",
       " 'unmasked (1449)',\n",
       " 'unmasked (1450)',\n",
       " 'unmasked (1451)',\n",
       " 'unmasked (1452)',\n",
       " 'unmasked (1453)',\n",
       " 'unmasked (1454)',\n",
       " 'unmasked (1455)',\n",
       " 'unmasked (1456)',\n",
       " 'unmasked (1457)',\n",
       " 'unmasked (1458)',\n",
       " 'unmasked (1459)',\n",
       " 'unmasked (1460)',\n",
       " 'unmasked (1461)',\n",
       " 'unmasked (1462)',\n",
       " 'unmasked (1463)',\n",
       " 'unmasked (1464)',\n",
       " 'unmasked (1465)',\n",
       " 'unmasked (1466)',\n",
       " 'unmasked (1467)',\n",
       " 'unmasked (1468)',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['257-with-mask',\n",
       " 'masked (6)',\n",
       " '399-with-mask',\n",
       " 'unmasked (1576)',\n",
       " 'masked (1332)']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_percentage = [50,30,20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_training = int(split_percentage[0] * size / 100)\n",
    "# split_validation = split_training + int(split_percentage[1] * size / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_validation = int(split_percentage[1] * size / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_test = int(split_percentage[2] * size / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['257-with-mask',\n",
       " 'masked (6)',\n",
       " '399-with-mask',\n",
       " 'unmasked (1576)',\n",
       " 'masked (1332)']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0:split_training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['418-with-mask',\n",
       " 'masked (1370)',\n",
       " 'unmasked (1649)',\n",
       " '270-with-mask',\n",
       " 'unmasked (1372)']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[split_training:(split_training+split_validation)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-with-mask',\n",
       " '217-with-mask',\n",
       " 'unmasked (1644)',\n",
       " 'unmasked (1584)',\n",
       " 'masked (1410)']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[split_training+split_validation:][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "\n",
    "def list_files(full_data_path ,  split_percentage ):\n",
    "    \n",
    "    files = []\n",
    "\n",
    "    discarded = 0\n",
    "    masked_instance = 0\n",
    "\n",
    "    for r, d, f in os.walk(full_data_path):\n",
    "        # print(r)\n",
    "        # print(d)\n",
    "        # print(f)\n",
    "        for file in f:\n",
    "            # print(file)\n",
    "            if file.endswith(\".txt\"):\n",
    "                # first, let's check if there is only one object\n",
    "                with open( full_data_path + 'obj/' + file, 'r') as fp: \n",
    "                    lines = fp.readlines()\n",
    "                    if len(lines) > 1:\n",
    "                        discarded += 1\n",
    "                        continue\n",
    "                # print(file)\n",
    "\n",
    "                strip = file[0:len(file) - len(\".txt\")]  \n",
    "                # print(strip)\n",
    "                # secondly, check if the paired image actually exist\n",
    "                # image_path = full_data_path + \"obj/\" + strip + '.jpg'\n",
    "                text_path = full_data_path + \"obj/\" + strip + '.txt'\n",
    "                # print(image_path)\n",
    "\n",
    "                if os.path.isfile(text_path):\n",
    "                    # checking the class. '0' means masked, '1' for unmasked\n",
    "                    # print('x')\n",
    "                    if lines[0][0] == '0':\n",
    "                        masked_instance += 1\n",
    "                    files.append(strip)\n",
    "\n",
    "\n",
    "    size = len(files)   \n",
    "    # print(str(discarded) + \" file(s) discarded\")\n",
    "    # print(str(size) + \" valid case(s)\")\n",
    "    # print(str(masked_instance) + \" are masked cases\")\n",
    "\n",
    "    random.shuffle(files)\n",
    "\n",
    "    split_training = int(split_percentage[0] * size / 100)\n",
    "    split_validation = int(split_percentage[1] * size / 100)\n",
    "    split_test = int(split_percentage[2] * size / 100)\n",
    "\n",
    "    return files[0:split_training], files[split_training:(split_training+split_validation)], files[split_training+split_validation:]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646 training files\n",
      "387 validation files\n",
      "259 test files\n"
     ]
    }
   ],
   "source": [
    "training_files, validation_files, test_files = list_files(full_data_path= full_data_path  , split_percentage = [50,30,20])\n",
    "\n",
    "print(str(len(training_files)) + \" training files\")\n",
    "print(str(len(validation_files)) + \" validation files\")\n",
    "print(str(len(test_files)) + \" test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'labeled-mask-dataset-yolo-darknet/versions/1/obj/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dir1_list = ['train', 'validation', 'test']\n",
    "sub_dir2_list = ['mask', 'without_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = Path('facemasks')\n",
    "# Create directories\n",
    "root_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_dir1 in sub_dir1_list:\n",
    "    sub_dir = root_dir / sub_dir1\n",
    "    sub_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sub_dir2 in sub_dir2_list:\n",
    "        sub_dir2 = sub_dir / sub_dir2\n",
    "        sub_dir2.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "file_path = Path(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\a27_YEARS_OLD\\deep_learning\\CV\\facemasks\\train\\image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_pattern = r\"^.*with-mask|masked.*$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_pattern = r\"^.*unmasked.*$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in files[0:split_training]:\n",
    "    if re.search(mask_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/train/mask')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[0:split_training]:\n",
    "    if re.search(unmasked_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/train/withoout_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[split_training:(split_training+split_validation)]:\n",
    "    if re.search(mask_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/validation/mask')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[split_training:(split_training+split_validation)]:\n",
    "    if re.search(unmasked_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/validation/without_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[split_training+split_validation:]:\n",
    "    if re.search(mask_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/test/mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[split_training+split_validation:]:\n",
    "    if re.search(unmasked_pattern, file):\n",
    "        img_file = file_path + file + \".jpg\"\n",
    "        shutil.copy(str(img_file), 'facemasks/test/without_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file\n",
    "try:\n",
    "    shutil.copy(str(file_path), str(sub_dir))\n",
    "    print(f\"File copied to '{sub_dir}'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect COVID-19 face masks in images\n",
    "Detect face masks in real-time video streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The number of images with facemask labelled ‘yes’ class: \n",
    "# The number of images with facemask labelled ‘no’ class: \n",
    "# We see that, after data augmentation, we have a total of 2751 images with \n",
    "# 1380 images in the ‘yes’ class and ‘1371’ images in the ‘no’ class.\n",
    "\n",
    "# Found 2200 images belonging to 2 classes. \n",
    "# Found 551 images belonging to 2 classes.\n",
    "\n",
    "# labels_dict={0:’without_mask’,1:’with_mask’} \n",
    "# color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "# ace_clsfr=cv2.CascadeClassifier(‘haarcascade_frontalface_default.xml’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "\ttf.keras.layers.Conv2D(100, (3, 3), activation='relu',\n",
    "\t\t\t\t\t\tinput_shape=(150, 150, 3)),\n",
    "\ttf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "\ttf.keras.layers.Conv2D(100, (3, 3), activation='relu'),\n",
    "\ttf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "\ttf.keras.layers.Flatten(),\n",
    "\ttf.keras.layers.Dropout(0.5),\n",
    "\ttf.keras.layers.Dense(50, activation='relu'),\n",
    "\ttf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagenerator_kwargs = dict(\n",
    "            rescale = 1./255,\n",
    "            # validation_split=0.20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator( # like CustomDataset for pytorch\n",
    "                rotation_range=40,\n",
    "                horizontal_flip=True,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                **datagenerator_kwargs\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_kwargs = dict(\n",
    "            target_size=[],\n",
    "            batch_size=32,\n",
    "            interpolation=\"bilinear\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatically detect labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagenerator.flow_from_directory( #  like Dataloader for pytorch\n",
    "            directory='facemasks/train',\n",
    "            subset=\"training\",\n",
    "            shuffle=True,\n",
    "            **dataflow_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automatically detect class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(**datagenerator_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory='facemasks/validation',\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoint.ckpt',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, # handle both X(features image) and y(labels)\n",
    "                    epochs=30,\n",
    "                    validation_data=valid_generator,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a real dataset (COCO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple dataset class\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, img_size=64):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Create a blank image\n",
    "        img = np.zeros((self.img_size, self.img_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Generate random bounding box\n",
    "        x_min = np.random.randint(5, self.img_size // 2) # This ensures the top-left corner stays in the upper-left quadrant of the image.\n",
    "        y_min = np.random.randint(5, self.img_size // 2) # This ensures the bottom-right corner stays in the lower-right quadrant of the image.\n",
    "        x_max = np.random.randint(x_min + 1, self.img_size - 5)\n",
    "        y_max = np.random.randint(y_min + 1, self.img_size - 5)\n",
    "\n",
    "        # Draw rectangle on image\n",
    "        img = cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (255, 255, 255), -1)\n",
    "        # img → The image on which the rectangle (bounding box) is drawn.\n",
    "# (x_min, y_min) → The top-left corner of the bounding box.\n",
    "# (x_max, y_max) → The bottom-right corner of the bounding box.\n",
    "# (255, 255, 255) → The color of the rectangle (in this case, white for an RGB image).\n",
    "# -1 → The thickness parameter\n",
    "\n",
    "        # Convert to tensor\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        # Classification label (1 = object, 0 = no object)\n",
    "        label = 1\n",
    "\n",
    "        # Bounding box [x_min, y_min, x_max, y_max] normalized\n",
    "        bbox = torch.tensor([x_min / self.img_size, y_min / self.img_size,\n",
    "                            x_max / self.img_size, y_max / self.img_size], dtype=torch.float32)\n",
    "        \n",
    "        return img_tensor, label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = ObjectDetectionDataset(num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ObjectDetectionDataset at 0x1cfd06187d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1cfb0e01ed0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        \n",
    "        # Shared CNN layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1), # 3 channels, 16 filters, 1 pixel shift, 1 pixel padding\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128) # flatten Dense layer 64*8*8 input neurons nodes and 128 outputs nodes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Classification head (binary)\n",
    "        self.classification_head = nn.Linear(128, 1) # The Dense layer outputs a single value\n",
    "        \n",
    "        # Bounding box head (x_min, y_min, x_max, y_max)\n",
    "        self.bbox_head = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x) #  torch.Size([32, 3, 28, 28])\n",
    "        x = x.view(x.shape[0], -1)  # Flatten torch.Size([32, 2352])\n",
    "\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        class_out = torch.sigmoid(self.classification_head(x))  # Binary classification\n",
    "        bbox_out = self.bbox_head(x)  # Bounding box regression\n",
    "\n",
    "        return class_out, bbox_out\n",
    "\n",
    "# Instantiate model\n",
    "model = ObjectDetectionModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss functions\n",
    "class_loss_fn = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "bbox_loss_fn = nn.MSELoss() # Mean Squared Error (MSE) Loss \n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Class Loss: 0.7097, BBox Loss: 0.2231\n",
      "Epoch [2/10], Class Loss: 0.6149, BBox Loss: 0.1104\n",
      "Epoch [3/10], Class Loss: 0.4780, BBox Loss: 0.0529\n",
      "Epoch [4/10], Class Loss: 0.3210, BBox Loss: 0.0587\n",
      "Epoch [5/10], Class Loss: 0.1244, BBox Loss: 0.2314\n",
      "Epoch [6/10], Class Loss: 0.1206, BBox Loss: 0.0616\n",
      "Epoch [7/10], Class Loss: 0.0746, BBox Loss: 0.0262\n",
      "Epoch [8/10], Class Loss: 0.0469, BBox Loss: 0.0323\n",
      "Epoch [9/10], Class Loss: 0.0363, BBox Loss: 0.0597\n",
      "Epoch [10/10], Class Loss: 0.0258, BBox Loss: 0.0465\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_class_loss = 0.0\n",
    "    epoch_bbox_loss = 0.0\n",
    "    \n",
    "    for images, labels, bboxes in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        class_preds, bbox_preds = model(images) # train image with bounding boxes\n",
    "\n",
    "        # Compute loss\n",
    "        class_loss = class_loss_fn(class_preds.squeeze(), labels.float()) # loss for class labels\n",
    "        bbox_loss = bbox_loss_fn(bbox_preds, bboxes) # loss for bboxes\n",
    "\n",
    "        total_loss = class_loss + bbox_loss\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_class_loss += class_loss.item()\n",
    "        epoch_bbox_loss += bbox_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Class Loss: {epoch_class_loss:.4f}, BBox Loss: {epoch_bbox_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a sample image\n",
    "sample_image, sample_label, sample_bbox = dataset[0]\n",
    "sample_image = sample_image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    pred_label, pred_bbox = model(sample_image)\n",
    "\n",
    "# Convert predictions to original scale\n",
    "H, W = dataset.img_size, dataset.img_size\n",
    "pred_bbox = pred_bbox.squeeze().numpy()\n",
    "pred_bbox = [int(pred_bbox[i] * H) for i in range(4)] # his contains normalized bounding box coordinates (values between 0 and 1).\n",
    "# H: The height (or width, assuming square images) of the original image.\n",
    "\n",
    "# Display result\n",
    "img = sample_image.squeeze().permute(1, 2, 0).numpy() # f sample_image has shape (1, C, H, W) (i.e., a batch of size 1),\n",
    "# .squeeze() removes the batch dimension, making it (C, H, W).PyTorch uses (C, H, W) format, whereas NumPy and Matplotlib expect (H, W, C).\n",
    "# (C, H, W) → (H, W, C)\n",
    "\n",
    "plt.imshow(img)\n",
    "# # Draw bounding box\n",
    "plt.gca().add_patch(plt.Rectangle((pred_bbox[0], pred_bbox[1]),  # # Top-left corner (x_min, y_min)\n",
    "                                pred_bbox[2] - pred_bbox[0],  # Width (x_max - x_min)\n",
    "                                pred_bbox[3] - pred_bbox[1],  # Height (y_max - y_min)\n",
    "\n",
    "                                edgecolor='red', linewidth=2, fill=False))\n",
    "plt.title(f\"Predicted Label: {int(pred_label.item() > 0.5)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images: train2017 and val2017 datasets\n",
    "\n",
    "Annotations: instances_train2017.json and instances_val2017.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO (Common Objects in Context) is a widely used dataset for object detection, segmentation, and keypoint detection tasks. Preparing COCO for object detection involves downloading the dataset, preprocessing images and annotations, and creating a DataLoader for training a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco_dataset/\n",
    "# │── train2017/             # contain all class images and labels are defined in annotations json format\n",
    "# │── val2017/\n",
    "# │── annotations/\n",
    "# │   ├── instances_train2017.json\n",
    "# │   ├── instances_val2017.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotation files (.json) contain bounding boxes and category labels. We can parse them using pycocotools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Define paths\n",
    "train_json = \"coco_dataset/annotations/instances_train2017.json\"\n",
    "val_json = \"coco_dataset/annotations/instances_val2017.json\"\n",
    "\n",
    "# Load COCO annotations\n",
    "coco_train = COCO(train_json)\n",
    "coco_val = COCO(val_json)\n",
    "\n",
    "# Get class labels\n",
    "categories = coco_train.loadCats(coco_train.getCatIds())\n",
    "category_names = {cat['id']: cat['name'] for cat in categories}\n",
    "print(category_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "annotation json sample format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "\n",
    "#   \"annotations\": [\n",
    "#     {\n",
    "#       \"id\": 1,\n",
    "#       \"image_id\": 1,\n",
    "#       \"category_id\": 1,\n",
    "#       \"bbox\": [50, 30, 100, 150],  // (x_min, y_min, width, height)\n",
    "#       \"area\": 15000,\n",
    "#       \"segmentation\": [[50,30, 150,30, 150,180, 50,180]],  // Optional for instance segmentation\n",
    "#       \"iscrowd\": 0\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": 2,\n",
    "#       \"image_id\": 1,\n",
    "#       \"category_id\": 2,\n",
    "#       \"bbox\": [200, 100, 80, 120],\n",
    "#       \"area\": 9600,\n",
    "#       \"segmentation\": [[200,100, 280,100, 280,220, 200,220]],\n",
    "#       \"iscrowd\": 0\n",
    "#     }\n",
    "#   ],\n",
    "#   \"categories\": [\n",
    "#     {\n",
    "#       \"id\": 1,\n",
    "#       \"name\": \"car\",\n",
    "#       \"supercategory\": \"vehicle\"\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": 2,\n",
    "#       \"name\": \"person\",\n",
    "#       \"supercategory\": \"human\"\n",
    "#     }\n",
    "#   ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load annotations (bounding boxes & labels)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Define dataset with transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((512, 512))\n",
    "])\n",
    "\n",
    "train_dataset = COCODataset(\"coco_dataset/train2017\", \"coco_dataset/annotations/instances_train2017.json\", transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# # for classification\n",
    "# # Assuming 'CustomDataset' returns (image, target) for each sample\n",
    "# train_dataset = CustomDataset(\"path/to/dataset\", transform=your_transform)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda batch: tuple(zip(*batch)))\n",
    "\n",
    "# Fetch a batch\n",
    "images, targets = next(iter(train_loader))\n",
    "\n",
    "# Print shape of first image & bounding boxes\n",
    "print(images[0].shape)\n",
    "print(targets[0]['boxes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_sample(image, target):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "    for box in target['boxes']:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first sample\n",
    "visualize_sample(images[0], targets[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an Object Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.detection as models\n",
    "\n",
    "# Load pre-trained Faster R-CNN\n",
    "model = models.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Modify the model for COCO dataset\n",
    "num_classes = 91  # COCO has 80 classes + 1 background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model using train_loader...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10  # Number of training epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, num_epochs=10):\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for images, targets in progress_bar:\n",
    "            images = [img.to(device) for img in images]  # Move images to GPU\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            loss_dict = model(images, targets)  # Forward pass\n",
    "            loss = sum(loss for loss in loss_dict.values())  # Sum up all losses\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert COCO Annotations to PyTorch Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Head: A single neuron with a sigmoid activation (for binary classification).\n",
    "\n",
    "Bounding Box Head: Four neurons for (x_min, y_min, x_max, y_max) prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "\n",
    "\t# grab the dimensions of the frame and \n",
    "\t# then construct a blob from it\n",
    "\t(h, w) = frame.shape[:2] #  height (h) and width (w) of the frame image.\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1.0, (224, 224), # DNN stands for Deep Neural Network\n",
    "\t\t\t\t\t\t\t\t(104.0, 177.0, 123.0))\n",
    "    # This line creates a blob (a binary large object) from the frame image, which is a format that can be fed into a DNN.\n",
    "    # frame: The input image frame.\n",
    "\t# 1.0: The scale factor for the image.\n",
    "\t# (224, 224): The desired size of the output blob.\n",
    "\t# (104.0, 177.0, 123.0): The mean values for each color channel (BGR) to be subtracted from the image.\n",
    "\t# pass the blob through the network \n",
    "\t# and obtain the face detections\n",
    "\tfaceNet.setInput(blob)\n",
    "\tdetections = faceNet.forward()\n",
    "\tprint(detections.shape)\n",
    "#  This line sets the input blob for the FaceNet model. The blob is a pre-processed image that has been \n",
    "# resized and normalized.\n",
    "# This line runs the FaceNet model on the input blob and stores the output in the detections variable.\n",
    "\n",
    "\t# initialize our list of faces, their\n",
    "\t# corresponding locations, and the list\n",
    "\t# of predictions from our face mask network\n",
    "\tfaces = []\n",
    "\tlocs = []\n",
    "\tpreds = []\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in range(0, detections.shape[2]): # where detections.shape[2] represents the number of detections.\n",
    "\t\n",
    "\t\t# extract the confidence (i.e.,\n",
    "\t\t# probability) associated with\n",
    "\t\t# the detection\n",
    "\t\tconfidence = detections[0, 0, i, 2] # This line extracts the confidence score (i.e., probability) associated with the current detection.\n",
    "\t\t#(batch_size, 1, num_detections, 7)\n",
    "# \t\t0: The batch size index (usually 0 for single-image inputs).\n",
    "# 0: The detection index (usually 0 for single-detection outputs).\n",
    "# i: The detection iteration index (loops over the number of detections).\n",
    "# 2: The confidence score index (usually 2 for FaceNet outputs).\n",
    "\t\t# filter out weak detections by \n",
    "\t\t# ensuring the confidence is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > 0.5:\n",
    "\t\t\n",
    "\t\t\t# compute the (x, y)-coordinates\n",
    "\t\t\t# of the bounding box for\n",
    "\t\t\t# the object\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h]) # This line extracts the bounding box coordinates (x, y, w, h) from the detections array.\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\") # This line scales the bounding box coordinates to the original image size by multiplying them with the image width and height.\n",
    "            # This line converts the bounding box coordinates to integers and unpacks them into separate variables (startX, startY, endX, endY).\n",
    "\t\t\t# ensure the bounding boxes fall \n",
    "\t\t\t# within the dimensions of\n",
    "\t\t\t# the frame\n",
    "\t\t\t(startX, startY) = (max(0, startX), max(0, startY))\n",
    "\t\t\t(endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "\t\t\t# extract the face ROI, convert it\n",
    "\t\t\t# from BGR to RGB channel\n",
    "\t\t\t# ordering, resize it to 224x224, \n",
    "\t\t\t# and preprocess it\n",
    "\t\t\tface = frame[startY:endY, startX:endX]\n",
    "\t\t\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "\t\t\tface = cv2.resize(face, (224, 224))\n",
    "\t\t\tface = img_to_array(face)\n",
    "\t\t\tface = preprocess_input(face)\n",
    "\n",
    "\t\t\t# add the face and bounding boxes \n",
    "\t\t\t# to their respective lists\n",
    "\t\t\tfaces.append(face)\n",
    "\t\t\tlocs.append((startX, startY, endX, endY))\n",
    "\n",
    "\t# only make a predictions if at least one\n",
    "\t# face was detected\n",
    "\tif len(faces) > 0:\n",
    "\t\n",
    "\t\t# for faster inference we'll make \n",
    "\t\t# batch predictions on *all*\n",
    "\t\t# faces at the same time rather \n",
    "\t\t# than one-by-one predictions\n",
    "\t\t# in the above `for` loop\n",
    "\t\tfaces = np.array(faces, dtype=\"float32\")\n",
    "\t\tpreds = maskNet.predict(faces, batch_size=32)\n",
    "\n",
    "\t# return a 2-tuple of the face locations\n",
    "\t# and their corresponding locations\n",
    "\treturn (locs, preds)\n",
    "\n",
    "\n",
    "# load our serialized face detector model from disk\n",
    "prototxtPath = r\"face_detector\\deploy.prototxt\"\n",
    "weightsPath = r\"face_detector\\res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load the face mask detector model from disk\n",
    "maskNet = load_model(\"mask_detector.model\")\n",
    "\n",
    "# initialize the video stream\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded \n",
    "\t# video stream and resize it\n",
    "\t# to have a maximum width of 400 pixels\n",
    "\tframe = vs.read()\n",
    "\tframe = imutils.resize(frame, width=400)\n",
    "\n",
    "\t# detect faces in the frame and \n",
    "\t# determine if they are wearing a\n",
    "\t# face mask or not\n",
    "\t(locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)\n",
    "\n",
    "\t# loop over the detected face \n",
    "\t# locations and their corresponding\n",
    "\t# locations\n",
    "\tfor (box, pred) in zip(locs, preds):\n",
    "\t\n",
    "\t\t# unpack the bounding box and predictions\n",
    "\t\t(startX, startY, endX, endY) = box\n",
    "\t\t(mask, withoutMask) = pred\n",
    "\n",
    "\t\t# determine the class label and \n",
    "\t\t# color we'll use to draw\n",
    "\t\t# the bounding box and text\n",
    "\t\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "\t\tcolor = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "\t\t# include the probability in the label\n",
    "\t\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "\t\t# display the label and bounding box \n",
    "\t\t# rectangle on the output frame\n",
    "\t\tcv2.putText(frame, label, (startX, startY - 10),\n",
    "\t\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting data\n",
    "\n",
    "Since our model has a fixed 244 x 244 input layer, we need to format any input image before feed it to the model (to train or to predict). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 244\n",
    "\n",
    "def format_image(img, box):\n",
    "    height, width = img.shape \n",
    "    max_size = max(height, width)\n",
    "    r = max_size / input_size\n",
    "    new_width = int(width / r)\n",
    "    new_height = int(height / r)\n",
    "    new_size = (new_width, new_height)\n",
    "    resized = cv.resize(img, new_size, interpolation= cv.INTER_LINEAR)\n",
    "    new_image = np.zeros((input_size, input_size), dtype=np.uint8)\n",
    "    new_image[0:new_height, 0:new_width] = resized\n",
    "\n",
    "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
    "    new_box = [int((x - 0.5*w)* width / r), int((y - 0.5*h) * height / r), int(w*width / r), int(h*height / r)]\n",
    "\n",
    "    return new_image, new_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(4, BATCH_SIZE//4, i + 1)\n",
    "        label = labels[0][i]\n",
    "        box = (labels[1][i] * input_size)\n",
    "        box = tf.cast(box, tf.int32)\n",
    "\n",
    "        image = images[i].numpy().astype(\"float\") * 255.0\n",
    "        image = image.astype(np.uint8)\n",
    "        image_color = cv.cvtColor(image, cv.COLOR_GRAY2RGB)\n",
    "\n",
    "        color = (0, 0, 255)\n",
    "        if label[0] > 0:\n",
    "            color = (0, 255, 0)\n",
    "\n",
    "        cv.rectangle(image_color, box.numpy(), color, 2)\n",
    "\n",
    "        plt.imshow(image_color)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model\n",
    "\n",
    "An Object Detection is a combination of two tasks:\n",
    "\n",
    "regression of the bound-box coordinates\n",
    "classification of the object label\n",
    "This means that our model has two outputs: namely the object label and the object bound box. Therefore, the model must combine the tasks of classification and regression.\n",
    "\n",
    "The Classifier\n",
    "\n",
    "Let’s forget the bound box for a while and begin with a simpler classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASSES = 2\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(224, 224, 1)),\n",
    "    tf.keras.layers.AveragePooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu'),\n",
    "    tf.keras.layers.AveragePooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu'),\n",
    "    tf.keras.layers.AveragePooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(CLASSES, activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CLASSES = 2\n",
    "\n",
    "def build_classifier(inputs):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(CLASSES, activation='softmax')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor(inputs):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units = '4')(inputs)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor(inputs):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 1))(inputs)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(32, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(64, kernel_size=3, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(2,2)(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_model_adaptor(inputs):\n",
    "    x = tf.keras.layers.Flatten()(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_classifier_head(inputs):\n",
    "    return tf.keras.layers.Dense(CLASSES, activation='softmax', name = 'classifier_head')(inputs)\n",
    "\n",
    "def build_regressor_head(inputs):\n",
    "    return tf.keras.layers.Dense(units = '4', name = 'regressor_head')(inputs)\n",
    "\n",
    "def build_model(inputs):\n",
    "    \n",
    "    feature_extractor = build_feature_extractor(inputs)\n",
    "\n",
    "    model_adaptor = build_model_adaptor(feature_extractor)\n",
    "\n",
    "    classification_head = build_classifier_head(model_adaptor)\n",
    "\n",
    "    regressor_head = build_regressor_head(model_adaptor)\n",
    "\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = [classification_head, regressor_head])\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
